# =============================================================================
# EasySql Configuration
# =============================================================================
# Copy this file to .env and fill in your actual values
# NEVER commit .env to version control!

# =============================================================================
# Neo4j Configuration
# =============================================================================
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=your_neo4j_password
# Neo4j database name (requires Neo4j 4.0+, default: neo4j)
# Use different databases for project isolation
NEO4J_DATABASE=neo4j

# =============================================================================
# Milvus Configuration
# =============================================================================
MILVUS_URI=http://localhost:19530
# Optional: For Milvus Cloud or authentication
# MILVUS_TOKEN=your_milvus_token
# Collection name prefix for isolation (e.g., "medical" -> "medical_table_embeddings")
MILVUS_COLLECTION_PREFIX=

# =============================================================================
# Embedding Model Configuration
# =============================================================================
# Provider type: local, openai_api, tei
#   - local: Local SentenceTransformer inference (default)
#   - openai_api: OpenAI-compatible API (vLLM, Ollama, LocalAI, Xinference)
#   - tei: HuggingFace Text Embeddings Inference
EMBEDDING_PROVIDER=local

# Model name (for local) or model identifier (for API)
# Recommended models for Chinese:
#   - BAAI/bge-large-zh-v1.5 (1024 dim, best quality)
#   - BAAI/bge-base-zh-v1.5 (768 dim, balanced)
#   - BAAI/bge-small-zh-v1.5 (512 dim, fastest)
EMBEDDING_MODEL=BAAI/bge-large-zh-v1.5
EMBEDDING_DIMENSION=1024

# Device for local inference: cpu, cuda, or leave empty for auto
# EMBEDDING_DEVICE=cuda

# Cache directory for local models (default: ~/.cache/huggingface)
# EMBEDDING_CACHE_DIR=./models

# --- API Provider Settings (only needed if EMBEDDING_PROVIDER != local) ---
# API base URL (required for openai_api and tei)
# EMBEDDING_API_BASE=http://localhost:11434/v1  # Ollama
# EMBEDDING_API_BASE=http://localhost:8000/v1   # vLLM
# EMBEDDING_API_BASE=http://localhost:8080      # TEI

# API key (optional, for services that require authentication)
# EMBEDDING_API_KEY=

# Request timeout in seconds
# EMBEDDING_TIMEOUT=60.0

# =============================================================================
# Source Database Configuration
# =============================================================================
# Format: DB_<NAME>_<PROPERTY>
# Supported types: mysql, postgresql

# Example MySQL database (HIS system)
DB_HIS_TYPE=mysql
DB_HIS_HOST=localhost
DB_HIS_PORT=3306
DB_HIS_USER=root
DB_HIS_PASSWORD=your_mysql_password
DB_HIS_DATABASE=his_db
DB_HIS_DESCRIPTION=医院信息管理系统

# Example PostgreSQL database (LIS system) - uncomment to enable
# DB_LIS_TYPE=postgresql
# DB_LIS_HOST=localhost
# DB_LIS_PORT=5432
# DB_LIS_USER=postgres
# DB_LIS_PASSWORD=your_pg_password
# DB_LIS_DATABASE=lis_db
# DB_LIS_SYSTEM_TYPE=LIS
# DB_LIS_DESCRIPTION=检验信息系统

# =============================================================================
# Pipeline Configuration
# =============================================================================
# Batch size for database operations
BATCH_SIZE=1000

# Enable/disable specific pipeline steps
ENABLE_SCHEMA_EXTRACTION=true
ENABLE_NEO4J_WRITE=true
ENABLE_MILVUS_WRITE=true

# =============================================================================
# Logging Configuration
# =============================================================================
LOG_LEVEL=INFO
LOG_FILE=logs/easysql.log

# =============================================================================
# Schema Retrieval Configuration (Text2SQL)
# =============================================================================
# Milvus search settings
RETRIEVAL_SEARCH_TOP_K=5
RETRIEVAL_EXPAND_FK=true
RETRIEVAL_EXPAND_MAX_DEPTH=1

# Semantic filter settings
SEMANTIC_FILTER_ENABLED=true
SEMANTIC_FILTER_THRESHOLD=0.5
SEMANTIC_FILTER_MIN_TABLES=3

# Core tables that should never be filtered (comma-separated)
CORE_TABLES=patient,employee,department,drug_dictionary,diagnosis_dictionary

# Bridge table protection
BRIDGE_PROTECTION_ENABLED=true
BRIDGE_MAX_HOPS=3

# =============================================================================
# LLM Filter Configuration (Optional - for highest precision)
# =============================================================================
# Enable LLM-based table filtering (adds latency but improves precision)
LLM_FILTER_ENABLED=false
LLM_FILTER_MAX_TABLES=8
LLM_FILTER_MODEL=deepseek-chat

# LLM API configuration (supports OpenAI-compatible APIs)
# For DeepSeek:
LLM_API_BASE=https://api.deepseek.com/v1
LLM_API_KEY=your_deepseek_api_key

# For OpenAI:
# LLM_API_BASE=https://api.openai.com/v1
# LLM_API_KEY=your_openai_api_key
# LLM_FILTER_MODEL=gpt-4o-mini

# =============================================================================
# LLM Layer Configuration (LangGraph Agent)
# =============================================================================

# Query Mode: 'plan' (Interactive/HITL) or 'fast' (Direct)
QUERY_MODE=plan

# --- Provider API Keys ---

# OpenAI Config
OPENAI_API_KEY=your_openai_api_key
OPENAI_API_BASE=https://api.openai.com/v1

# Google Gemini Config
GOOGLE_API_KEY=your_google_ai_studio_key

# Anthropic Config
ANTHROPIC_API_KEY=your_anthropic_api_key

# MCP / Tool Config
# Path to DBHub config file for stdio MCP transport (spawns subprocess)
# MCP_DBHUB_CONFIG=./dbhub.toml

# --- Model Selection ---
# Provider is auto-detected based on model priority: Google > Anthropic > OpenAI
# The first provider with BOTH a model AND API key configured will be used.

# Google Gemini model (highest priority if GOOGLE_API_KEY is set)
# GOOGLE_LLM_MODEL=gemini-1.5-pro

# Anthropic Claude model (used if ANTHROPIC_API_KEY is set and no Google model)
# ANTHROPIC_LLM_MODEL=claude-3-5-sonnet-20241022

# OpenAI model (fallback, always used if no other provider configured)
OPENAI_LLM_MODEL=gpt-4o

# Optional: Model for plan mode's analysis/clarification phase
# If not set, falls back to the primary model above
# MODEL_PLANNING=gpt-4o-mini

# LLM sampling temperature (0.0 - 2.0)
# Set to 1.0 for models/endpoints that require fixed temperature (e.g., Kimi 2.5).
LLM_TEMPERATURE=0.0

# =============================================================================
# Code Context Configuration (DDD Support)
# =============================================================================
# Enable code context retrieval for Text2SQL
# When enabled, DDD domain code (entities, enums) will be retrieved
# alongside schema information to provide business context
CODE_CONTEXT_ENABLED=false

# Number of code entities/enums to retrieve
CODE_CONTEXT_SEARCH_TOP_K=5
CODE_CONTEXT_ENUM_TOP_K=10

# Minimum relevance score for code retrieval
CODE_CONTEXT_SCORE_THRESHOLD=0.3

# Maximum code snippets to include in LLM context
CODE_CONTEXT_MAX_SNIPPETS=3

# Cache directory for file hash tracking (incremental updates)
CODE_CONTEXT_CACHE_DIR=.code_context_cache

# Supported languages (comma-separated)
CODE_CONTEXT_SUPPORTED_LANGUAGES=csharp,python,java,javascript,typescript

# Collection name prefix for code context (e.g., "medical" -> "medical_code_chunks")
# If not set, falls back to default collection name "code_chunks"
CODE_CONTEXT_COLLECTION_PREFIX=

# =============================================================================
# Few-Shot Configuration
# =============================================================================
# Enable retrieval of similar historical Q&A examples for in-context learning
FEW_SHOT_ENABLED=false

# Maximum number of few-shot examples to include in prompt
FEW_SHOT_MAX_EXAMPLES=3

# Minimum vector similarity score for retrieval (0.0 - 1.0)
FEW_SHOT_MIN_SIMILARITY=0.6

# Milvus collection name for few-shot examples
FEW_SHOT_COLLECTION_NAME=medical_few_shot_examples

# =============================================================================
# LangFuse Observability (Optional)
# =============================================================================
# Enable LangFuse tracing for LLM observability
# Sign up at https://cloud.langfuse.com or self-host
LANGFUSE_ENABLED=false
LANGFUSE_SECRET_KEY=sk-xxxx
LANGFUSE_PUBLIC_KEY=pk-xxxxx
LANGFUSE_BASE_URL=http://localhost:4000
# Preferred (Langfuse SDK standard): LANGFUSE_BASE_URL
# Backward compatibility (optional): LANGFUSE_HOST=https://your-langfuse-instance.com

# =============================================================================
# Checkpointer Configuration (LangGraph State Persistence)
# =============================================================================
# Backend: "memory" (development) or "postgres" (production)
CHECKPOINTER_BACKEND=memory

# PostgreSQL settings (only used when CHECKPOINTER_BACKEND=postgres)
CHECKPOINTER_POSTGRES_HOST=localhost
CHECKPOINTER_POSTGRES_PORT=5432
CHECKPOINTER_POSTGRES_USER=postgres
CHECKPOINTER_POSTGRES_PASSWORD=111111
CHECKPOINTER_POSTGRES_DATABASE=easysql

# Connection pool settings
# CHECKPOINTER_POOL_MIN_SIZE=1
# CHECKPOINTER_POOL_MAX_SIZE=10

# =============================================================================
# Session Store (PostgreSQL Only)
# =============================================================================
# Session storage requires PostgreSQL. Use an async SQLAlchemy URI.
# SESSION_BACKEND=postgres
# SESSION_POSTGRES_URI=postgresql+asyncpg://postgres:password@localhost:5432/easysql
