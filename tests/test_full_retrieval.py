#!/usr/bin/env python
"""
å®Œæ•´é“¾è·¯æµ‹è¯•ï¼šä½¿ç”¨ SchemaRetrievalService è¿›è¡Œæ™ºèƒ½æ£€ç´¢ï¼Œå¹¶é›†æˆ ContextBuilder ç”Ÿæˆ LLM ä¸Šä¸‹æ–‡ï¼Œæœ€åè°ƒç”¨ LLM ç”Ÿæˆ SQL

æµ‹è¯•å®Œæ•´çš„æ£€ç´¢æµç¨‹ï¼š
    Query â†’ Milvusæœç´¢ â†’ FKæ‰©å±• â†’ è¯­ä¹‰è¿‡æ»¤ â†’ æ¡¥æ¢ä¿æŠ¤ â†’ LLMè£å‰ª(å¯é€‰) â†’ Contextæ„å»º â†’ LLMç”ŸæˆSQL

è¿è¡Œï¼š
    PYTHONPATH=. python tests/test_full_retrieval.py
"""

import os
import re

from dotenv import load_dotenv
from openai import OpenAI

from easysql.context import ContextBuilder, ContextInput
from easysql.embeddings import EmbeddingService
from easysql.readers.milvus_reader import MilvusSchemaReader
from easysql.readers.neo4j_reader import Neo4jSchemaReader
from easysql.repositories.milvus_repository import MilvusRepository
from easysql.repositories.neo4j_repository import Neo4jRepository
from easysql.retrieval import (
    RetrievalConfig,
    SchemaRetrievalService,
)

# ä» .env åŠ è½½é…ç½®
load_dotenv()

MILVUS_URI = os.getenv("MILVUS_URI", "http://localhost:19530")
MILVUS_COLLECTION_PREFIX = os.getenv("MILVUS_COLLECTION_PREFIX", "medical")
NEO4J_URI = os.getenv("NEO4J_URI", "bolt://localhost:7687")
NEO4J_USER = os.getenv("NEO4J_USER", "neo4j")
NEO4J_PASSWORD = os.getenv("NEO4J_PASSWORD", "")
DB_NAME = "medical"  # æºæ•°æ®åº“åï¼Œç”¨äºéš”ç¦»

# LLM é…ç½®
LLM_API_BASE = os.getenv("LLM_API_BASE", "https://api.moonshot.cn/v1")
LLM_API_KEY = os.getenv("LLM_API_KEY", "")
LLM_MODEL = os.getenv("LLM_SQL_MODEL", "kimi-k2-0905-preview")

# æµ‹è¯•é—®é¢˜åˆ—è¡¨
TEST_QUESTIONS = [
    ("ç®€å•", "æ‚£è€…ä¿¡æ¯", ["patient"]),
    ("ä¸­ç­‰", "æŸ¥è¯¢ä½é™¢è¶…è¿‡7å¤©çš„æ‚£è€…", ["admission", "patient"]),
    (
        "å¤æ‚",
        "æŸ¥è¯¢æ‚£è€…çš„å¤„æ–¹ã€ç”¨è¯å’Œè´¹ç”¨æ˜ç»†",
        ["patient", "prescription", "prescription_detail", "fee_record"],
    ),
    (
        "å¤æ‚",
        "æ‰¾å‡ºåšè¿‡CTæ£€æŸ¥çš„ä½é™¢æ‚£è€…åŠå…¶ä¸»æ²»åŒ»ç”Ÿ",
        ["inspection_request", "admission", "patient", "employee"],
    ),
]


def generate_sql(
    client: OpenAI,
    system_prompt: str,
    user_prompt: str,
    model: str = LLM_MODEL,
) -> str:
    """
    è°ƒç”¨ LLM ç”Ÿæˆ SQL è¯­å¥ã€‚

    Args:
        client: OpenAI å®¢æˆ·ç«¯
        system_prompt: ç³»ç»Ÿæç¤ºè¯
        user_prompt: ç”¨æˆ·æç¤ºè¯ï¼ˆåŒ…å« schema å’Œé—®é¢˜ï¼‰
        model: æ¨¡å‹åç§°

    Returns:
        ç”Ÿæˆçš„ SQL è¯­å¥
    """
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.0,  # ä½¿ç”¨ç¡®å®šæ€§è¾“å‡º
            max_tokens=1024,
        )

        content = response.choices[0].message.content

        # æå– SQLï¼ˆå¤„ç† markdown ä»£ç å—ï¼‰
        sql = content.strip()

        # å¦‚æœè¿”å›çš„æ˜¯ markdown ä»£ç å—ï¼Œæå–å…¶ä¸­çš„ SQL
        if "```sql" in sql.lower():
            match = re.search(r"```sql\s*(.*?)\s*```", sql, re.DOTALL | re.IGNORECASE)
            if match:
                sql = match.group(1).strip()
        elif "```" in sql:
            match = re.search(r"```\s*(.*?)\s*```", sql, re.DOTALL)
            if match:
                sql = match.group(1).strip()

        return sql

    except Exception as e:
        return f"-- Error: {str(e)}"


def main():
    print("=" * 70)
    print("SchemaRetrievalService å®Œæ•´é“¾è·¯æµ‹è¯•")
    print("=" * 70)

    # 1. åˆå§‹åŒ–æœåŠ¡
    print("\n[1] åˆå§‹åŒ–æœåŠ¡...")
    embedding_service = EmbeddingService.create_local(model_name="BAAI/bge-large-zh-v1.5")

    milvus_repo = MilvusRepository(
        uri=MILVUS_URI,
        collection_prefix=MILVUS_COLLECTION_PREFIX,
    )
    milvus_repo.connect()

    neo4j_repo = Neo4jRepository(
        uri=NEO4J_URI,
        user=NEO4J_USER,
        password=NEO4J_PASSWORD,
    )
    neo4j_repo.connect()

    milvus_reader = MilvusSchemaReader(
        repository=milvus_repo,
        embedding_service=embedding_service,
    )

    neo4j_reader = Neo4jSchemaReader(
        repository=neo4j_repo,
    )

    # åˆå§‹åŒ– LLM å®¢æˆ·ç«¯
    llm_client = None
    if LLM_API_KEY:
        llm_client = OpenAI(
            api_key=LLM_API_KEY,
            base_url=LLM_API_BASE,
        )
        print(f"    âœ… LLM å®¢æˆ·ç«¯å·²åˆå§‹åŒ– (model: {LLM_MODEL})")
    else:
        print("    âš ï¸ LLM_API_KEY æœªè®¾ç½®ï¼Œè·³è¿‡ SQL ç”Ÿæˆ")

    # 2. åˆ›å»ºæ£€ç´¢æœåŠ¡ (ä½¿ç”¨é…ç½®)
    config = RetrievalConfig(
        # Milvus æœç´¢
        search_top_k=5,
        # FK æ‰©å±•
        expand_fk=True,
        expand_max_depth=1,
        # è¯­ä¹‰è¿‡æ»¤ (å…³é”®é…ç½®)
        semantic_filter_enabled=False,
        semantic_threshold=0.55,
        semantic_min_tables=3,
        # æ ¸å¿ƒè¡¨ç™½åå• (è¿™äº›è¡¨ä¸ä¼šè¢«è¿‡æ»¤)
        # core_tables=["patient", "employee", "department"],
        # æ¡¥æ¢è¡¨ä¿æŠ¤
        bridge_protection_enabled=True,
        bridge_max_hops=3,
        # LLM è£å‰ª (å¯é€‰ï¼Œéœ€è¦è®¾ç½® API key)
        llm_filter_enabled=True,
        llm_api_key=os.getenv("LLM_API_KEY"),
        llm_api_base=os.getenv("LLM_API_BASE", "https://api.moonshot.cn/v1"),
        llm_filter_model=os.getenv("LLM_FILTER_MODEL", "kimi-k2-0905-preview"),
        llm_filter_max_tables=8,
    )

    service = SchemaRetrievalService(
        milvus_reader=milvus_reader,
        neo4j_reader=neo4j_reader,
        config=config,
    )

    # æ˜¾ç¤ºé…ç½®
    print("\n[2] æ£€ç´¢é…ç½®:")
    print(f"    ğŸ” search_top_k: {config.search_top_k}")
    print(f"    ğŸ”„ expand_fk: {config.expand_fk}")
    print(
        f"    ğŸ“Š semantic_filter: {config.semantic_filter_enabled} (threshold={config.semantic_threshold})"
    )
    print(f"    ğŸ”— bridge_protection: {config.bridge_protection_enabled}")
    print(f"    ğŸ¤– llm_filter: {config.llm_filter_enabled}")
    print(f"    ğŸ“Œ core_tables: {config.core_tables}")

    # 3. æµ‹è¯•æ¯ä¸ªé—®é¢˜
    total_coverage = 0
    total_expected = 0

    for level, question, expected_tables in TEST_QUESTIONS:
        print("\n" + "=" * 70)
        print(f"[{level}] ğŸ” é—®é¢˜: {question}")
        print(f"ğŸ“Œ æœŸæœ›è¡¨: {expected_tables}")
        print("=" * 70)

        # æ‰§è¡Œæ£€ç´¢
        result = service.retrieve(question=question, db_name=DB_NAME)

        # æ˜¾ç¤ºç»Ÿè®¡
        stats = result.stats

        # Milvus æœç´¢ç»Ÿè®¡
        milvus_stats = stats.get("milvus_search", {})
        print(f"\nğŸ“‹ Step 1: Milvus è¯­ä¹‰æœç´¢ ({milvus_stats.get('count', 0)} å¼ è¡¨)")
        milvus_tables = milvus_stats.get("tables", [])
        milvus_scores = milvus_stats.get("scores", {})
        for i, t in enumerate(milvus_tables[:5], 1):
            hit = "âœ…" if t in expected_tables else "  "
            score = milvus_scores.get(t, 0)
            print(f"   {hit} {i}. {t} (score: {score:.4f})")

        # FK æ‰©å±•ç»Ÿè®¡
        fk_stats = stats.get("fk_expansion", {})
        if fk_stats:
            print(
                f"\nğŸ”„ Step 2: FK æ‰©å±• ({fk_stats.get('before', 0)} â†’ {fk_stats.get('after', 0)} å¼ )"
            )
            added = fk_stats.get("added", [])
            if added:
                print(f"   æ–°å¢: {added[:8]}{'...' if len(added) > 8 else ''}")

        # è¿‡æ»¤ç»Ÿè®¡
        filter_stats = stats.get("filters", {})
        if "chain" in filter_stats:
            chain = filter_stats["chain"]

            # è¯­ä¹‰è¿‡æ»¤
            if "semantic" in chain:
                sem = chain["semantic"]
                print("\nğŸ“Š Step 3: è¯­ä¹‰è¿‡æ»¤")
                print(
                    f"   ä¿ç•™: {sem.get('after', '?')} å¼  (å¿…ä¿: {sem.get('must_keep', 0)}, é«˜åˆ†: {sem.get('kept_by_score', 0)})"
                )
                removed = sem.get("removed", [])
                if removed:
                    print(f"   ç§»é™¤ä½åˆ†è¡¨: {removed[:5]}{'...' if len(removed) > 5 else ''}")

            # æ¡¥æ¢ä¿æŠ¤
            if "bridge" in chain:
                bridge = chain["bridge"]
                bridges_added = bridge.get("bridges_added", [])
                if bridges_added:
                    print(f"\nğŸ”— Step 4: æ¡¥æ¢ä¿æŠ¤ (æ·»åŠ  {len(bridges_added)} å¼ )")
                    print(f"   æ¡¥æ¢è¡¨: {bridges_added}")

            # LLM è£å‰ª
            if "llm" in chain:
                llm = chain["llm"]
                if llm.get("action") == "llm_filter":
                    print("\nğŸ¤– Step 5: LLM è£å‰ª")
                    print(f"   æ¨¡å‹: {llm.get('model', 'N/A')}")
                    print(f"   {llm.get('before', '?')} â†’ {llm.get('after', '?')} å¼ è¡¨")
                elif llm.get("action") == "skipped":
                    print(f"\nğŸ¤– Step 5: LLM è£å‰ª (è·³è¿‡: {llm.get('reason', 'N/A')})")

        # æœ€ç»ˆç»“æœ
        print(f"\nğŸ“‹ æœ€ç»ˆè¡¨åˆ—è¡¨ ({len(result.tables)} å¼ ):")
        for i, t in enumerate(result.tables, 1):
            hit = "âœ…" if t in expected_tables else "  "
            print(f"   {hit} {i}. {t}")

        # è¦†ç›–ç‡æ£€æŸ¥
        found = set(result.tables) & set(expected_tables)
        missing = set(expected_tables) - set(result.tables)
        coverage = len(found) / len(expected_tables) * 100
        total_coverage += len(found)
        total_expected += len(expected_tables)

        print(
            f"\n   è¦†ç›–ç‡: {len(found)}/{len(expected_tables)} ({coverage:.0f}%) | ç¼ºå¤±: {list(missing) or 'æ— '}"
        )

        # JOIN è·¯å¾„
        if result.join_paths:
            print(f"\nğŸ”— JOIN è·¯å¾„ ({len(result.join_paths)} æ¡):")
            for edge in result.join_paths[:5]:
                print(
                    f"   â€¢ {edge['fk_table']}.{edge['fk_column']} â†’ {edge['pk_table']}.{edge['pk_column']}"
                )
            if len(result.join_paths) > 5:
                print(f"   ... è¿˜æœ‰ {len(result.join_paths) - 5} æ¡")

        # ===== Context æ„å»º =====
        print(f"\n{'=' * 70}")
        print("ğŸ“ Context æ„å»ºæµ‹è¯•")
        print("=" * 70)

        # åˆ›å»º ContextInput
        context_input = ContextInput(
            question=question,
            retrieval_result=result,
            db_name=DB_NAME,
        )

        # ä½¿ç”¨é»˜è®¤çš„ ContextBuilder æ„å»ºä¸Šä¸‹æ–‡
        builder = ContextBuilder.default()
        context_output = builder.build(context_input)

        # è¾“å‡º Context ç»Ÿè®¡
        print("\nğŸ“Š Context ç»Ÿè®¡:")
        print(f"   æ€» Token æ•°: {context_output.total_tokens}")
        print(f"   Section æ•°é‡: {len(context_output.sections)}")
        for section in context_output.sections:
            print(f"     - {section.name}: {section.token_count} tokens")

        # è¾“å‡º System Prompt
        print(f"\n{'â”€' * 70}")
        print("ğŸ¤– System Prompt:")
        print("â”€" * 70)
        print(context_output.system_prompt)

        # è¾“å‡º User Prompt
        print(f"\n{'â”€' * 70}")
        print("ğŸ‘¤ User Prompt:")
        print("â”€" * 70)
        print(context_output.user_prompt)
        print("â”€" * 70)

        # ===== LLM ç”Ÿæˆ SQL =====
        if llm_client:
            print(f"\n{'=' * 70}")
            print("ğŸ§  LLM SQL ç”Ÿæˆ")
            print("=" * 70)

            sql = generate_sql(
                client=llm_client,
                system_prompt=context_output.system_prompt,
                user_prompt=context_output.user_prompt,
                model=LLM_MODEL,
            )

            print("\nğŸ“ ç”Ÿæˆçš„ SQL:")
            print("â”€" * 70)
            print(sql)
            print("â”€" * 70)

    # 4. æ€»ç»“
    total_pct = total_coverage / total_expected * 100 if total_expected > 0 else 0
    print("\n" + "=" * 70)
    print(f"æµ‹è¯•å®Œæˆï¼æ€»è¦†ç›–ç‡: {total_coverage}/{total_expected} ({total_pct:.0f}%)")
    print("=" * 70)

    # 5. å…³é—­è¿æ¥
    milvus_repo.close()
    neo4j_repo.close()

if __name__ == "__main__":
    main()
